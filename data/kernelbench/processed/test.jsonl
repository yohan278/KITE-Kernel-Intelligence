{"task_id": "L3_44", "level": 3, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # regularization\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n    \nclass Model(nn.Module):\n    \"\"\" an unassuming Transformer block \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(n_embd)\n        self.attn = CausalSelfAttention(n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen)\n        self.ln_2 = nn.LayerNorm(n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc    = nn.Linear(n_embd, 4 * n_embd),\n            c_proj  = nn.Linear(4 * n_embd, n_embd),\n            act     = NewGELU(),\n            dropout = nn.Dropout(resid_pdrop),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]", "reference_kernel": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # regularization\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n    \nclass Model(nn.Module):\n    \"\"\" an unassuming Transformer block \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(n_embd)\n        self.attn = CausalSelfAttention(n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen)\n        self.ln_2 = nn.LayerNorm(n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc    = nn.Linear(n_embd, 4 * n_embd),\n            c_proj  = nn.Linear(4 * n_embd, n_embd),\n            act     = NewGELU(),\n            dropout = nn.Dropout(resid_pdrop),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]", "metadata": {"source": "kernelbench_dataset_api", "level": 3, "problem_id": 44, "problem_name": "44_MiniGPTBlock.py", "ref_arch_src": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # regularization\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n    \nclass Model(nn.Module):\n    \"\"\" an unassuming Transformer block \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(n_embd)\n        self.attn = CausalSelfAttention(n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen)\n        self.ln_2 = nn.LayerNorm(n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc    = nn.Linear(n_embd, 4 * n_embd),\n            c_proj  = nn.Linear(4 * n_embd, n_embd),\n            act     = NewGELU(),\n            dropout = nn.Dropout(resid_pdrop),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]"}}
{"task_id": "L3_45", "level": 3, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\nimport torch\nimport torch.nn as nn\n\n# U-Net Implementation\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, features):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param features: Number of base features (will be doubled in each layer)\n        \"\"\"\n        super(Model, self).__init__()\n        self.encoder1 = DoubleConv(in_channels, features)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = DoubleConv(features, features * 2)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = DoubleConv(features * 2, features * 4)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = DoubleConv(features * 4, features * 8)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.bottleneck = DoubleConv(features * 8, features * 16)\n\n        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n        self.decoder4 = DoubleConv(features * 16, features * 8)\n        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n        self.decoder3 = DoubleConv(features * 8, features * 4)\n        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n        self.decoder2 = DoubleConv(features * 4, features * 2)\n        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n        self.decoder1 = DoubleConv(features * 2, features)\n\n        self.final_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(self.pool1(enc1))\n        enc3 = self.encoder3(self.pool2(enc2))\n        enc4 = self.encoder4(self.pool3(enc3))\n\n        bottleneck = self.bottleneck(self.pool4(enc4))\n\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        return self.final_conv(dec1)\n    \nbatch_size = 8\nin_channels = 8\nout_channels = 4\nheight = 64\nwidth = 512\nfeatures = 64\n# Test code for UNet\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, features]\n", "reference_kernel": "import torch\nimport torch.nn as nn\n\n# U-Net Implementation\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, features):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param features: Number of base features (will be doubled in each layer)\n        \"\"\"\n        super(Model, self).__init__()\n        self.encoder1 = DoubleConv(in_channels, features)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = DoubleConv(features, features * 2)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = DoubleConv(features * 2, features * 4)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = DoubleConv(features * 4, features * 8)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.bottleneck = DoubleConv(features * 8, features * 16)\n\n        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n        self.decoder4 = DoubleConv(features * 16, features * 8)\n        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n        self.decoder3 = DoubleConv(features * 8, features * 4)\n        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n        self.decoder2 = DoubleConv(features * 4, features * 2)\n        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n        self.decoder1 = DoubleConv(features * 2, features)\n\n        self.final_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(self.pool1(enc1))\n        enc3 = self.encoder3(self.pool2(enc2))\n        enc4 = self.encoder4(self.pool3(enc3))\n\n        bottleneck = self.bottleneck(self.pool4(enc4))\n\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        return self.final_conv(dec1)\n    \nbatch_size = 8\nin_channels = 8\nout_channels = 4\nheight = 64\nwidth = 512\nfeatures = 64\n# Test code for UNet\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, features]\n", "metadata": {"source": "kernelbench_dataset_api", "level": 3, "problem_id": 45, "problem_name": "45_UNetSoftmax.py", "ref_arch_src": "import torch\nimport torch.nn as nn\n\n# U-Net Implementation\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, features):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param features: Number of base features (will be doubled in each layer)\n        \"\"\"\n        super(Model, self).__init__()\n        self.encoder1 = DoubleConv(in_channels, features)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = DoubleConv(features, features * 2)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = DoubleConv(features * 2, features * 4)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = DoubleConv(features * 4, features * 8)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.bottleneck = DoubleConv(features * 8, features * 16)\n\n        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n        self.decoder4 = DoubleConv(features * 16, features * 8)\n        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n        self.decoder3 = DoubleConv(features * 8, features * 4)\n        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n        self.decoder2 = DoubleConv(features * 4, features * 2)\n        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n        self.decoder1 = DoubleConv(features * 2, features)\n\n        self.final_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(self.pool1(enc1))\n        enc3 = self.encoder3(self.pool2(enc2))\n        enc4 = self.encoder4(self.pool3(enc3))\n\n        bottleneck = self.bottleneck(self.pool4(enc4))\n\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        return self.final_conv(dec1)\n    \nbatch_size = 8\nin_channels = 8\nout_channels = 4\nheight = 64\nwidth = 512\nfeatures = 64\n# Test code for UNet\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, features]\n"}}
{"task_id": "L3_46", "level": 3, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n", "reference_kernel": "# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n", "metadata": {"source": "kernelbench_dataset_api", "level": 3, "problem_id": 46, "problem_name": "46_NetVladWithGhostClusters.py", "ref_arch_src": "# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n"}}
{"task_id": "L3_47", "level": 3, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 0\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n", "reference_kernel": "# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 0\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n", "metadata": {"source": "kernelbench_dataset_api", "level": 3, "problem_id": 47, "problem_name": "47_NetVladNoGhostClusters.py", "ref_arch_src": "# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 0\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n"}}
{"task_id": "L3_48", "level": 3, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        states = new_states[:, :-1]\n        \n        # 4. Compute state-to-output conversion\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', \n                           C_blocks, states, state_decay_out)\n        \n        # Combine diagonal and off-diagonal terms\n        Y = rearrange(Y_diag + Y_off, \"b c l h p -> b (c l) h p\")\n        \n        \n        return Y\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n", "reference_kernel": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        states = new_states[:, :-1]\n        \n        # 4. Compute state-to-output conversion\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', \n                           C_blocks, states, state_decay_out)\n        \n        # Combine diagonal and off-diagonal terms\n        Y = rearrange(Y_diag + Y_off, \"b c l h p -> b (c l) h p\")\n        \n        \n        return Y\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n", "metadata": {"source": "kernelbench_dataset_api", "level": 3, "problem_id": 48, "problem_name": "48_Mamba2ReturnY.py", "ref_arch_src": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        states = new_states[:, :-1]\n        \n        # 4. Compute state-to-output conversion\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', \n                           C_blocks, states, state_decay_out)\n        \n        # Combine diagonal and off-diagonal terms\n        Y = rearrange(Y_diag + Y_off, \"b c l h p -> b (c l) h p\")\n        \n        \n        return Y\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n"}}
{"task_id": "L3_49", "level": 3, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        return new_states[:, -1]\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n", "reference_kernel": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        return new_states[:, -1]\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n", "metadata": {"source": "kernelbench_dataset_api", "level": 3, "problem_id": 49, "problem_name": "49_Mamba2ReturnFinalState.py", "ref_arch_src": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        return new_states[:, -1]\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n"}}
{"task_id": "L3_50", "level": 3, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass Model(nn.Module):\n    \"\"\"\n    A multi-head masked self-attention layer with a projection at the end that uses ReLU instead of Softmax.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.relu(att)\n\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        return y\n\nbatch_size = 16\nmax_seqlen = 1024\nn_embd = 768  # Hidden dimension, typical for BERT-base size\nn_head = 12   # Number of attention heads, typical for BERT-base size\n\ndef get_inputs():\n    return [torch.rand(batch_size, max_seqlen, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, max_seqlen]", "reference_kernel": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass Model(nn.Module):\n    \"\"\"\n    A multi-head masked self-attention layer with a projection at the end that uses ReLU instead of Softmax.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.relu(att)\n\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        return y\n\nbatch_size = 16\nmax_seqlen = 1024\nn_embd = 768  # Hidden dimension, typical for BERT-base size\nn_head = 12   # Number of attention heads, typical for BERT-base size\n\ndef get_inputs():\n    return [torch.rand(batch_size, max_seqlen, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, max_seqlen]", "metadata": {"source": "kernelbench_dataset_api", "level": 3, "problem_id": 50, "problem_name": "50_ReLUSelfAttention.py", "ref_arch_src": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass Model(nn.Module):\n    \"\"\"\n    A multi-head masked self-attention layer with a projection at the end that uses ReLU instead of Softmax.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.relu(att)\n\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        return y\n\nbatch_size = 16\nmax_seqlen = 1024\nn_embd = 768  # Hidden dimension, typical for BERT-base size\nn_head = 12   # Number of attention heads, typical for BERT-base size\n\ndef get_inputs():\n    return [torch.rand(batch_size, max_seqlen, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, max_seqlen]"}}
{"task_id": "L4_1", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 1, "problem_name": "1_EleutherAI-gpt-neo-2p7B_bs32_seq256.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_2", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 2, "problem_name": "2_facebook-opt-1p3b_bs1_seq2047.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_3", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 3, "problem_name": "3_EleutherAI-gpt-neo-2p7B_bs1_seq2047.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_4", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 4, "problem_name": "4_facebook-opt-1p3b_bs32_seq256.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_5", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 4095\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 4095\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 5, "problem_name": "5_google-bigbird-roberta-base_bs1_seq4095.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 4095\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_6", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 6, "problem_name": "6_facebook-bart-large_bs1_seq1023.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_7", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 7, "problem_name": "7_gpt2_bs32_seq256.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_8", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 8, "problem_name": "8_facebook-opt-1p3b_bs512_seq32.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_9", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 9, "problem_name": "9_google-bigbird-roberta-base_bs32_seq256.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_10", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 10, "problem_name": "10_google-bigbird-roberta-base_bs1024_seq32.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_11", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 511\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 511\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 11, "problem_name": "11_google-electra-small-discriminator_bs1_seq511.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 511\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_12", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 12, "problem_name": "12_google-electra-small-discriminator_bs1024_seq32.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_13", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 13, "problem_name": "13_google-reformer-enwik8_bs32_seq256.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_14", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 14, "problem_name": "14_google-electra-small-discriminator_bs32_seq256.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_15", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 15, "problem_name": "15_google-reformer-enwik8_bs1024_seq32.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_16", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 16, "problem_name": "16_gpt2_bs1_seq1023.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_17", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 17, "problem_name": "17_facebook-bart-large_bs1024_seq32.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_18", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 18, "problem_name": "18_EleutherAI-gpt-neo-2p7B_bs512_seq32.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_19", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 19, "problem_name": "19_gpt2_bs1024_seq32.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
{"task_id": "L4_20", "level": 4, "prompt": "Optimize this PyTorch model with a custom GPU kernel implementation.\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "reference_kernel": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "metadata": {"source": "kernelbench_dataset_api", "level": 4, "problem_id": 20, "problem_name": "20_facebook-bart-large_bs32_seq256.py", "ref_arch_src": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]"}}
